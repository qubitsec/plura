# 필상 시스템 — Day 12 복원력(Resilience)·DR/BCP 설계

1. **날짜**

* 2024-08-21

2. **목표**

* **장애 허용 설계**(RTO/RPO)와 **DR/BCP 절차** 확정
* 리전 분리(kr/jp)·데이터 주권을 유지하면서 **영속데이터의 무손실/최소손실 복구** 보장

---

3. **내용**

### A. 목표치(SLO 연동)

* **RTO(복구시간)**: 콘솔/API 30분, 인입 파이프라인 15분, 검색/리포트 60분
* **RPO(복구시점)**: 메타-DB ≤ 5분, 사건/티켓 ≤ 5분, 인덱스(검색) ≤ 15분, 객체 증거 0(불변·버전드)
* **장애 감지→조치**: 10분 이내 1차 완화(트래픽 감쇠/롤백), 30분 내 서비스 정상화

### B. DR 계층(서비스별)

| 계층         | 구성요소                                 | 방식                           | 비고               |
| ---------- | ------------------------------------ | ---------------------------- | ---------------- |
| **핵심 제어면** | API Gateway/OIDC, Config/FeatureFlag | **이중 AZ 액티브/액티브**            | L7 Health + mTLS |
| **스트림/큐**  | Kafka/Redis Streams(DLQ 포함)          | **AZ 다중 브로커** + ISR/복제2      | 오프셋 스냅샷 일 1회     |
| **메타-DB**  | Postgres                             | **동일 리전 동기 레플리카** + 비동기 세컨더리 | RLS 유지           |
| **검색**     | Solr + ZK                            | 샤드 Replication(2) + ZK 3노드   | 스냅샷/레플리케이션       |
| **객체 증거**  | MinIO/S3                             | **버전닝+WORM(불변)**             | KMS로 암호화         |
| **관측**     | OTel/Tempo/Loki/Grafana              | 멀티AZ + 장기저장                  | SLO 게이트          |

.  

> 원칙: **리전 간 고객데이터 복제 금지(주권 준수)**. 단, **메타-카탈로그/릴리스산출물**은 글로벌 공유.

### C. 백업·스냅샷 표준

* **Postgres**: PITR(연속 WAL) + 시간별 베이스 백업, 보존 14일
* **Solr**: 컬렉션 스냅샷(시간별), 보존 7일 → Object Storage 업로드
* **MinIO/S3**: 버전닝+WORM, 라이프사이클(3년)
* **구성/IaC**: Git 원장 + OCI 레지스트리(이미지) · 차트/오버레이 아카이브
* **키/비밀**: KMS 키 메타데이터는 리전 내 백업, **키 복제 없음**(주권 준수)

### D. 장애 시나리오 & 대응(요약)

1. **AZ 단절(kr-a)** → L4/L7 헬스체크 실패 → 트래픽 **kr-b/c**로 재분배(자동)
2. **메타-DB 프라이머리 장애** → 동기 레플리카 자동 승격 + 앱 `readiness` 게이트로 보호
3. **Solr 샤드 리더 손실** → 리더 재선출, 실패 시 스냅샷에서 **샤드 단위 복구**
4. **스트림 지연/적체** → 소비자 수 자동 증설, 불가 시 **샘플링 완화 정책** 적용
5. **객체 저장소 장애** → 버전드 리플리카 엔드포인트로 Failover(동일 리전)
6. **구성 오배포** → FeatureFlag **즉시 롤백** + `stable/*` 이미지로 Blue-Green 복귀

### E. 리전 DR 전략(주권 보존형)

* **교차 리전**:

  * **공유 가능**: 릴리스 아티팩트, 대시보드 템플릿, 룰/모델 바이너리(데이터 없음)
  * **불가/기본 차단**: 고객 원로그/사건/PII
* **옵션(고객 서면 동의 시)**: **라벨·카디널리티 축소된 익명 텔레메트리**(운영 품질지표)만 jp↔kr 상호 복제
* **페일오버**: 전체 서비스 크로스리전 전환은 **비상 시 경영진 승인** 후, **데이터 이행 없이** 제한적 콘솔 가용성만 제공

### F. 복구 절차(런북 핵심)

1. **Incident 선언**(심각도 분류·롤) → 온콜 페이지
2. **트래픽 완화**: 게이트웨이 레이트·버스트 제한, Canary 강등/롤백
3. **데이터 계층 복구**

   * Postgres: 타임라인 확인 → `promote` → PITR 필요 시 목표 시점으로 복구
   * Solr: 리더 재선출 실패 시 스냅샷 Restore → 재인덱스 큐 배수 확인
   * MinIO: 버전 ID 기준 **정합성 검증**(무결성 해시 비교)
4. **검증**: 합성 트랜잭션(로그 인입→사건→알림) **p95 지연** 정상화 확인
5. **사후**: RCA(근본원인/재발방지) 48h 내 완료, SLO·에러버짓 보고

### G. 카오스/DR 드릴(정기)

* **월 1회** AZ 격리 실험, **분기 1회** 메타-DB 승격/복구 연습, **반기 1회** 검색 복구(스냅샷→재인덱스)
* **성공 기준**: RTO/RPO 달성, 알람/런북 준수, 데이터 무결성 해시 불일치 0건

### H. 용량·예비 리소스

* **핫 스탠바이**: 각 핵심 서비스 1× 여유(평시 30% 여유 헤드룸)
* **버스트 플랜**: 인입 급증 시 **샘플링/TTL 단축** 자동 정책, 장기적으로 노드 증설

### I. 커뮤니케이션 & 고객 통지

* **상태페이지**: 리전 상태·가용성·진행 중 조치, ETA 대신 **사실/지표 중심**
* **고객 알림**: 중요 장애 시 60분 내 1보, 24h 내 RCA 요약, 변경/완화 조치 안내

---

4. **예시 설명 (KR 리전 AZ 장애 → 18분 내 정상화)**

1) `kr-a` 네트워크 이슈로 **게이트웨이/메타-DB** 헬스 실패 → **ALERT** 발화, Incident 선언
2) 트래픽 **kr-b/c**로 자동 재분배, **Canary 서비스 강등**(신규 릴리스 일시 중단)
3) 메타-DB 프라이머리 다운 → **동기 레플리카 승격(2분)**, 애플리케이션 연결 자동 복구
4) Solr 일부 샤드 리더 재선출, 적체 증가 → 소비자 증설 + **샘플링 10%** 임시 완화(인입 지연 방어)
5) 합성 트랜잭션 체크: 인입→사건 p95 13s, 알림 p95 8s로 회복 → 트래픽 정책 원복
6) 총 **18분** 내 서비스 정상화, DLQ 잔여 3k 건은 순차 재처리. 48h 내 **RCA** 발행 및 재발방지(네트워크 헬스 프루빙 강화, ZK 코터 조정) 반영

> 결과: 리전 내 멀티AZ 설계와 자동 승격/스냅샷 복구 절차로 **RTO/RPO 목표를 충족**, 데이터 주권·무결성을 유지한 채 안정적으로 서비스 연속성을 확보.
