# 필상 시스템 — Day 59 메시징/이벤트 버스 운영 표준 (Kafka/Redis Streams)

1. **날짜**

* 2024-11-01

2. **목표**

* **멀티테넌트·리전 분리** 전제에서 **토픽/스트림 설계, Exactly-once 효과, 재처리·백프레셔, 보안/감사** 표준 확정
* 인입→탐지→사건→알림 전 경로의 **지연 p95 ≤ 300ms**, **유실 0**, **중복률 ≤ 0.1%** 달성

---

3. **내용**

### A. 토픽/스트림 네이밍 & 파티션

```
kafka:  {region}.{domain}.{event}.{vN}
예) kr.ingest.log.v1, kr.detect.verdict.v1, kr.incident.change.v1, kr.notify.request.v1
redis:  {region}:{domain}:{stream}:{vN}
```

* **파티션 키**: `tenant_id`(기본) → 균형/격리, 서브키 `incident_key`(순서 보장 구간)
* **샤딩 기준**: 피크 RPS, 파티션당 목표 처리량 `≤ 2k msg/s`, 복제계수 3, acks=all

### B. 메시지 규격(헤더/페이로드)

* **필수 헤더**: `event_id(ULID)`, `tenant_id`, `region`, `occurred_at`, `produced_at`, `schema_version`, `trace_id`
* **멱등 키**: `idempotency_key = sha256(tenant_id|occurred_at|primary_fields)`
* **페이로드**: JSON/CBOR, **JSON Schema** 레지스트리 `events/{domain}/{event}/vN.json`로 검증(EMC 원칙 연계)

### C. Exactly-once 효과(EoS) & 트랜잭션

* **프로듀서**: `enable.idempotence=true`, `max.in.flight.requests.per.connection=1`, `retries=∞`, `delivery.timeout.ms=120000`
* **컨슈머**: **프로세싱+오프셋 커밋**을 트랜잭션으로 결합(Outbox/Inbox 패턴)
* **Outbox 패턴**: DB 트랜잭션 내 `outbox` 테이블에 이벤트 기록 → **증분 폴러**가 카프카 전송(재전송 안전)
* **Inbox 디듀프**: `idempotency_key`를 KV 캐시에 **TTL=24h**로 저장 후 재처리 방지

### D. 재처리·리플레이·DLQ

* **재시도 단계**: 즉시(3회, 지수 백오프 1s/5s/30s) → `*.retry` 토픽(지연큐) → 실패 시 `*.dlq`
* **DLQ 스키마**: 원 메시지 + `error_type`, `stack`, `first_seen`, `attempts`, `last_error_ts`
* **리플레이 API**: `POST /v1/streams/dlq/{topic}:replay?window=…&limit=…` (스키마 재검증/파티션 균형)

### E. 백프레셔 & 유량 제어

* **프로듀서**: `linger.ms=5–15`, `batch.size` 튜닝, **압축 zstd**
* **컨슈머**: 동시 처리량 자동 스케일(HPA) + **레이트 리밋**(테넌트/토픽)
* **혼잡 신호**: 레그(consumer lag), 처리 지연 p95, DLQ 증가율 → **샘플링/필드 축약 모드** 자동 전환

### F. 보존/컴팩션/규정 준수

* **실시간 토픽**: 7일 보존, `min.insync.replicas=2`
* **컴팩션 토픽**(상태 스냅샷): `cleanup.policy=compact,delete`, 보존 30일
* **주권/보안**: 리전 간 복제 금지, SASL/SCRAM + mTLS, **ACL=tenant 단위**(프로듀스/컨슘 분리)

### G. 상관·오케스트레이션 훅

* **릴리스/플래그 이벤트**는 `ops.change.v1`로 발행 → 스트림 처리기들이 **동적 파라미터** 갱신
* **SLO 게이트 연동**: 인입→사건/사건→알림 지연 p95 임계 초과 시 **Canary 강등/샘플링 상승** 트리거

### H. 관측/지표 & SLO

* **브로커/토픽**: `in/out bytes`, `records/sec`, `under_replicated`, `offline_partitions`, `request_queue`
* **컨슈머**: `lag`, `max.lag`, `rebalances`, `process_latency_p95`, `dlq_size`
* **목표**: 브로커 오류율 <0.5%, 컨슈머 재밸런스 시간 p95 < 5s, **엔드투엔드 지연 p95 ≤ 300ms**

### I. 런북(핵심 시나리오)

1. **Lag 급증**: 컨슈머 스케일↑ → 파티션 편향 확인 → 병목 파티션 리밸런스/증설
2. **DLQ 폭증**: 공통 스키마 실패? → 레지스트리 롤백 or 핫픽스 → `*.retry`로 단계적 재생
3. **브로커 장애**: ISR↓/오프라인 파티션 발생 → 리더 재선출 모니터링, 필요 시 **수동 재할당**
4. **크로스-테넌트 접근 시도**: ACL 히트 + 감사 이벤트 → 즉시 차단, 보안 티켓 자동 생성

### J. API/자동화(발췌)

* `POST /v1/streams/publish { topic, headers, payload }`
* `GET  /v1/streams/metrics?topic=…&window=5m`
* `POST /v1/streams/tenants/{id}:grant { topic, role: "produce|consume" }`
* `POST /v1/streams/topic:rotate { topic, action:"rollover|compact|retire" }`

---

4. **예시 설명 (KR 리전 DLQ 폭증 — 32분 내 정상화)**

1) `kr.detect.verdict.v1`에 스키마 v2 릴리스 후 DLQ 급증(타임스탬프 형식 불일치).
2) 레지스트리 핫픽스(v2.1) 적용 + 컨슈머 **스키마 호환 모드** `READ`로 하향.
3) `*.retry`로 단계적 리플레이(배수 0.5×) → lag 0.8M → **0**(32분) 회복, 중복률 0.07%.
4) 원인: 프로듀서 일부 인스턴스 구버전. **Outbox 스키마 가드** 보강, 릴리스 체크리스트 업데이트.
   → 결과: **EoS+DLQ/리트라이+스키마 가드**로 유실 없이 신속 복구, SLO·주권·테넌트 격리를 유지.
